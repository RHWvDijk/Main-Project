{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Model 'Xception' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.applications.xception import Xception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val','train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val','valid')\n",
    "\n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "train_gen, val_gen = get_pcam_generators('C:/Users/Daniel/Documents/')\n",
    "input = Input(input_shape)\n",
    "# get the pretrained model, cut out the top layer\n",
    "pretrained = Xception(input_shape=input_shape, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pretrained.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "layernames = []\n",
    "for i,layer in enumerate(pretrained.layers):\n",
    "    layernames.append(layer.name)\n",
    "    \n",
    "\n",
    "for j in range(14,0,-1): #verander hier het aantal blokken dat je pretrained model heeft\n",
    "    string = 'block'+str(j)\n",
    "    indices = [i for i, s in enumerate(layernames) if string in s]      \n",
    "    for layer in pretrained.layers[min(indices):]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = pretrained(input)\n",
    "    output = GlobalAveragePooling2D()(output)\n",
    "    output = Dropout(0.5)(output)\n",
    "    output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "    # Check the trainable status of the individual layers\n",
    "    #for layer in pretrained.layers:\n",
    "    #    print(layer, layer.trainable)\n",
    "\n",
    "    model = Model(input, output)\n",
    "    model.compile(SGD(lr=0.001, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "    #\n",
    "    # print a summary of the model on screen\n",
    "    print(string)\n",
    "\n",
    "    model.summary()\n",
    "        \n",
    "    # save the model and weights\n",
    "    model_name = 'Xception weights ImageNet '+string\n",
    "    model_filepath = model_name + '.json'\n",
    "    weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "    model_json = model.to_json() # serialize model to JSON\n",
    "    with open(model_filepath, 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "\n",
    "    # define the model checkpoint and Tensorboard callbacks\n",
    "    checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "    callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "    # train the model, note that we define \"mini-epochs\"\n",
    "    train_steps = train_gen.n//train_gen.batch_size//20\n",
    "    val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "    # since the model is trained for only 10 \"mini-epochs\", i.e. half of the data is\n",
    "    # not used during training\n",
    "\n",
    "    history = model.fit_generator(train_gen, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=10,\n",
    "                        callbacks=callbacks_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
